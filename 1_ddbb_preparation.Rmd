---
title: "*Plantae* database"
output: html_notebook
---

# Summary

This section explains in detail the preparation of a database with the GBIF data for *Plantae*, the Terrestrial Ecoregions of the World, country limits, and environmental data. The main objective is to find out what plant species are present on each ecoregion. 

# Data

## *Plantae* data from GBIF

Done from the website, with the DOI [https://doi.org/10.15468/dl.xh5y5g](https://doi.org/10.15468/dl.xh5y5g), (URL [https://www.gbif.org/es/occurrence/download/0061691-200613084148143](https://www.gbif.org/es/occurrence/download/0061691-200613084148143)).

The query had the following filtering criteria:

Basis of record: Observación, Observación humana, Muestra de material, Literatura, Espécimen preservado, Observación con máquina, and Espécimen vivo.
Includes coordinates: true
Geospatial issues: false
Occurrence status: present
Scientific name: Plantae

It results in 244830168 records from 4741 datasets downloaded in a file named *0061691-200613084148143.zip*. The size of the csv of the same name is 131GB. 


## Terrestrial Ecoregions of the World (TEOW)

This dataset is stored at `data/ecoregions/wwf_terr_ecos.shp`, and generated by by Olson *et al*. (2001), is hosted at [https://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world](https://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world). It is a shapefile with 867 terrestrial ecoregions classified into 14 biomes.

The key attributes of this dataset are:

 +  OBJECTID: unique identifier for each ecoregion.
 +  AREA: total area of the ecoregion.
 +  PERIMETER: total perimeter of the ecoregion.
 +  ECO_NAME: name of the ecoregion.
 +  BIOME: biome the ecoregion belongs to.
 
 
## Limits of the world countries

This shapefile is stored in `data/countries/countries.shp`, and contains the limits of the countries of the world. The key columns are:

 +  OBJECTID: unique identifier for each country
 +  ISO: ISO acronym of the country.
 +  NAME_ENGLI: country name.


# Preparing the data

## Setting up the working environment

We first install and load several R packages.

```{r, include = FALSE}
#automatic install of packages if they are not readily available
list.of.packages <- c(
  "tidyverse", 
  "sparklyr", 
  "data.table", 
  "dtplyr", 
  "dbplyr", 
  "RPostgreSQL",
  "readr"
  )

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages) > 0){
  install.packages(new.packages, dep=TRUE)
}

library(sparklyr)
library(dplyr)
library(dtplyr)
library(dbplyr)
library(data.table)
library(RPostgreSQL)
library(readr)
```

The package `sparklyr` is an `R` interface to [Apache Spark](https://spark.apache.org/). Spark is an engine for large-data processing, and allows `R` to work with files larger than the RAM memory of a computer. It does so by separating a dataset into pieces (chunks) that are written into the hard disk. Data processing in Spark is done in parallel, and each data chunk is managed independently by a *worker*. Spark datasets are named DataFrames, but they have nothing to do with the traditional data frames used by `R`. However, Spark DataFrames can easily be processed with `dplyr` through `sparklyr`. Since Spark DataFrames are stored in the hard disk, data processing operations are still relatively slow, but much faster than in a traditional database. 

### Setting up Spark

The package `sparklyr` helps to install Spark easily. The version install is `3.0.0` because it is compatible with `Java 11`, installed in my computer.

```{r, eval = TRUE, echo = FALSE}
#installs spark
sparklyr::spark_install(version = "3.0.0")

#checks installed version
sparklyr::spark_install_find()
```

During the Spark configuration it is very important to give it the location of the PostgreSQL driver (downloaded from the PostgreSQL webpage).

```{r}
#settting total Spark memory
# Sys.setenv("SPARK_MEM" = "20g")

#acquiring the configuration
spark.config <- sparklyr::spark_config()

#adding postgresql driver
spark.config$`sparklyr.shell.driver-class-path` <- "/home/blas/Dropbox/GITHUB/gbif_plantae/postgresql-42.2.16.jar"

#type of deployment
spark.config$`sparklyr.shell.deploy-mode` <- "client"

#memory management
spark.config$`sparklyr.shell.driver-memory` <- "28G"
# spark.config$`sparklyr.shell.executor-memory` <- '3.7G'
spark.config$spark.memory.fraction <- 0.1
spark.config$spark.dynamicAllocation.enabled <- "true"
spark.config$spark.memory.offHeap.enabled <- "true"
spark.config$spark.memory.offHeap.size <- "20G"

#establishing the connection
spark.connection <- sparklyr::spark_connect(
  master = "local",
  config = spark.config,
  version = "3.0.0"
  )
```

Before starting, it is quite advisable to read ["10 things I wish someone had told me before I started using Apache SparkR"](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/1792412399382575/3601578643761083/latest.html?utm_campaign=Open%20Source&utm_source=Databricks%20Blog).

The spark GUI can either be opened with:

```{r, eval = FALSE}
sparklyr::spark_web(spark.connection)
```

or by writing `http://localhost:4040` in the web browser.

### Setting up PostgreSQL and PostGIS

From synaptic, install `postgresql-12`, `postgresql-12-postgis-3`, `postgresql-contrib`, and `pgadmin3`.

PostgreSQL has a default user named `postgres`. This user can create and access any database in the local server. By default it has no password. Here we define the password `postgres` for the user `postgres`.

`sudo -u postgres psql -c "ALTER USER postgres PASSWORD 'postgres';"`

To configure the server, from the system shell we first create the user and password. In this case I use my name and my personal password.

`sudo -u postgres createuser --interactive`

The database files will live in the folder `/var/lib/postgresql/12/main` (the actual path can be found by executing `SHOW data_directory;` in psql or Pgadmin4), where 12 is the version number of PostgreSQL installed in the system. If that folder is in a drive with not enough space to host the database, another data folder can be defined as follows.

```{bash}
#stopping postgresql server
sudo systemctl stop postgresql

#check that it actually worked
sudo systemctl status postgresql

#open postgresql.conf
sudo gedit /etc/postgresql/12/main/postgresql.conf
```

In the text editor, replace `data_directory = '/var/lib/postgresql/12/main'` with `data_directory = '/media/workshop/posstgresql_data/main'`, and restart the postgresql server.

```{bash}
sudo systemctl start postgresql
sudo systemctl status postgresql
```


Creating the project database

`sudo -u blas createdb flora_ecoregions`

Getting into the `flora_ecoregions` database:

`psql flora_ecoregions blas`

To activate the postgis extension, for the database `flora_ecoregions` execute these SQL clauses in psql (if it doesn't work, it can be done later from the *Query tool* of Pgadmin4):

```{sql connection=spark.connection, eval = FALSE}
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_raster;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION postgis_sfcgal;
SELECT PostGIS_version();
```

The last clause should yield `3.0 USE_GEOS=1 USE_PROJ=1 USE_STATS=1` if the extension is up and running.

In case you need to check the version of PostgreSQL running in your system, from psql just type `select version();`.

## Setting up Pgadmin4

Pgadmin4 is a GUI to manage PostgreSQL databases. To install it we first need to add the software repository of the PostgreSQL Global Development Group (PGDG):

```{bash, eval = FALSE}
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -

echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" |sudo tee  /etc/apt/sources.list.d/pgdg.list

sudo apt update

sudo apt install pgadmin4 pgadmin4-apache2
```

The install requires an email address and an admin password.

After the install, check if the service is running.

```{bash, eval = FALSE}
systemctl status apache2
```

Firewall rules need to be set:

```{bash, eval = FALSE}
sudo ufw allow http
sudo ufw allow https
```

To connect with the local server, create a new connection with the IP 127.0.0.1, the user `postgres` and the password `postgres`.

### Connecting R and PostgreSQL

A good reference on how to do this can be found here [https://db.rstudio.com/dplyr/](https://db.rstudio.com/dplyr/). The packages `dbplyr` and `RPostgreSQL` are required to establish this connection. The function `RPostgreSQL::dbConnect()` does the job.

```{r}
postgresql.connection <- RPostgreSQL::dbConnect(
  drv = "PostgreSQL",
  dbname = "flora_ecoregions",
  user = "blas",
  password = rstudioapi::askForPassword("Database password"),
  port = 5432
)
```

We can list the tables available with  `RPostgreSQL::dbListTables()`.

```{r, eval = FALSE}
RPostgreSQL::dbListTables(postgresql.connection)
```

## Data processing with Spark

### Reading the *plantae* dataset into Spark

The function `sparklyr::spark_read_csv()` can be used to read a big csv file into Spark as follows. The reading and data fragmentation process takes a while.

```{r, echo = TRUE}
plantae <- sparklyr::spark_read_csv(
  spark.connection, 
  name = "plantae", 
  path = "data/plantae/plantae.csv", 
  header = TRUE, 
  delimiter = "\t"
)
```

`TIP: name R data.frames with the prefix 'rdf' and SparkR DataFrames with the prefix 'sdf'.`

To check that the table is loaded into the spark session we use `dplyr::src_tbls()`.

```{r}
dplyr::src_tbls(spark.connection)
```

###Removing uneeded columns and renaming others

Columns can be dropped and renamed with `dplyr::select()`. 

```{r}
plantae <- dplyr::select(
  plantae,
  kingdom = kingdom,
  phylum = phylum,
  class = class,
  family = family,
  genus = genus,
  species = species,
  latitude = decimalLatitude,
  longitude = decimalLongitude,
  coordinate_uncertainty = coordinateUncertaintyInMeters,
  year = year,
  taxon_key = taxonKey,
  species_key = speciesKey,
  basis_of_record = basisOfRecord,
  issue = issue
  )
colnames(plantae)
```

###Rounding coordinates to the third digit

When rounding coordinates to the third digit, coordinates that are meters apart become the same, and therefore redundancies can be removed by eliminating duplicates. 

```{r}
plantae <- dplyr::mutate(
  plantae,
  latitude = round(latitude, 3),
  longitude = round(longitude, 3)
)
```


###Removing duplicates

GBIF data generally has a large number of duplicates. These can be removed with `sparklyr::sdf_drop_duplicates()`, by defining a set of columns over which to look for duplicates.

```{r, eval = FALSE, echo = FALSE}
plantae <- sparklyr::sdf_drop_duplicates(
  plantae,
  cols = c(
  "genus",
  "species",
  "latitude",
  "longitude"
  )
)
```

###Filtering out rows with NA

The data has rows with `NA` for the `species` and `genus` column. To remove these, we use `dplyr::filter()`.

```{r}
plantae <- dplyr::filter(
  plantae,
  !is.na(species),
  !is.na(genus)
)
```


### Replacing NaN with 0 in the column `coordinate_uncertainty`

The column `coordinate_uncertainty` informs about the quality of the geolocation, but most records have no values, that were imported as `NaN` into the Spark DataFrame. Since `dplyr::filter()` removes data with NA, I need to give some value to those `NaN` to avoid filtering out these records.

```{r}
plantae <- sparklyr::na.replace(
  plantae,
  coordinate_uncertainty  = 0
  )
```

Now I can remove records with a `coordinate_uncertainty` larger than 10km.

```{r}
plantae <- dplyr::filter(
  plantae,
  coordinate_uncertainty <= 10000
)
```


Since dplyr + spark use what's named *lazy evaluation*, any data processing only occurs when the data is requested. Therefore, something like 

```{r}
head(plantae)
```

will start to execute the `dplyr` commands shown above.

###Saving a copy of the original data into the `flora_ecoregions` database.

The function `sparklyr::spark_write_jdbc()` can write the `plantae` table into the `flora_ecoregions` database as follows. Notice that during the configuration of Spark, shown above, the PostgreSQL driver was downloaded from the PostgreSQL website, and its location was given to the Spark configuration file. Here the driver used needs to be mentioned again (see the `driver` argument of `sparklyr::spark_write_jdbc()`). This operation takes time.

```{r, eval = TRUE}
sparklyr::spark_write_jdbc(
  x = plantae,
  name = "plantae",
  options = list(
    url = "jdbc:postgresql://localhost:5432/flora_ecoregions",
    user = "postgres",
    password = "postgres",#rstudioapi::askForPassword("Database password"),
    driver = "org.postgresql.Driver"
  )
)
```

To check the size of the database once the table has been written into PostgreSQL you can use the following SQL clause, that gives the size in MB. 

```{sql connection=postgresql.connection, eval = TRUE}
select pg_database_size('flora_ecoregions')/1024/1024;
```

Alternatively, the psql console can be used, as follows.

```{bash}
sudo -u postgres psql
\l+ flora_ecoregions
```


To read the table back to Spark [CRASHED MY SESSION...]

```{r, eval = FALSE}
plantae <- sparklyr::spark_read_jdbc(
  sc = spark.connection,
  name = "plantae",
  options = list(
    url = "jdbc:postgresql://localhost:5432/flora_ecoregions",
    user = "postgres",
    password = rstudioapi::askForPassword("Database password"),
    driver = "org.postgresql.Driver",
    dbtable = "plantae"
  )
)
```



The connection with Spark can be closed now.

```{r}
sparklyr::spark_disconnect(spark.connection)
```

**TIP: dropping a database**

To drop a database

```{sql connection=postgresql.connection, eval = FALSE}
-- Connecting to the current user localhost's postgres instance
sudo -u postgres psql

-- Making sure the database exists
SELECT * from pg_database where datname = 'flora_ecoregions'

-- Disallow new connections
UPDATE pg_database SET datallowconn = false WHERE datname = 'flora_ecoregions';
ALTER DATABASE flora_ecoregions CONNECTION LIMIT 1;

-- Terminate existing connections
SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'flora_ecoregions';

-- Drop database
DROP DATABASE flora_ecoregions
```


## Preparing the plantae table

The table needs to have a unique identifier for each row, and an index in such column

```{sql connection=postgresql.connection, eval = TRUE}
ALTER TABLE plantae ADD COLUMN id bigserial PRIMARY KEY;
CREATE INDEX plantae_id_index ON plantae (id);
```

There are records with null values in the latitude and longitude column

### geom column from latitude and longitude

```{sql connection=postgresql.connection}
-- Adding geometry column and geom field
SELECT AddGeometryColumn ('plantae','geom',4326,'POINT',2);
UPDATE plantae SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326);

-- Generating spatial index  
CREATE INDEX plantae_geom_index ON plantae USING GIST (geom);
```


## Working with ecoregions

The world's ecoregions are the working units of this project. To import `wwf_terr_ecos.shp` into the `flora_ecoregions`, we first convert the shp into sql with `shp2pgsql`, which comes installed with PostGIS.

```{bash, echo = FALSE}
shp2pgsql -s 4326 -I /home/blas/Dropbox/RESEARCH/DATA/Global_bio_ecoregions_Dinerstein_2017/Ecoregions2017.shp ecoregions > ecoregions.sql
```

Now, `ecoregions.sql` must be executed in the PostgreSQL server.

```{r}
RPostgreSQL::dbSendQuery(
  conn = postgresql.connection,
  statement = readr::read_file('ecoregions.sql')
)
```

We list the tables again to check that one named `ecoregions` exists.

```{r}
RPostgreSQL::dbListTables(postgresql.connection)
```


We can check this table from the R environment to decide what columns we need to remove.

```{r, echo = FALSE}
ecoregions_db <- RPostgreSQL::dbReadTable(
  con = postgresql.connection, 
  name = "ecoregions"
  )
```

We can delete the columns directly in the database

```{sql connection=postgresql.connection}
ALTER TABLE ecoregions
DROP COLUMN gid,
DROP COLUMN objectid,
DROP COLUMN biome_num,
DROP COLUMN eco_biome_,
DROP COLUMN nnh,
DROP COLUMN eco_id,
DROP COLUMN color,
DROP COLUMN color_bio,
DROP COLUMN color_nnh,
DROP COLUMN license;
```

And rename several columns

```{sql connection=postgresql.connection}
ALTER TABLE ecoregions RENAME COLUMN eco_name TO ecoregion;
ALTER TABLE ecoregions RENAME COLUMN biome_name TO biome;
ALTER TABLE ecoregions RENAME COLUMN shape_leng TO perimeter;
ALTER TABLE ecoregions RENAME COLUMN shape_area TO area;
ALTER TABLE ecoregions RENAME COLUMN nnh_name TO conservation_assessment;
```


```{r}
rm(ecoregions_db)
```

As with the table `plantae`, `ecoregions` needs a unique ID as primary key, an index on this column, and an index in the geom column.

```{sql connection=postgresql.connection, eval = FALSE}
ALTER TABLE ecoregions ADD COLUMN id serial PRIMARY KEY;
CREATE INDEX ecoregions_id_index ON ecoregions (id);
```

And a spatial index on geom

```{sql connection=postgresql.connection}
CREATE INDEX ecoregions_geom_index ON ecoregions USING GIST (geom);
```


## Joining ecoregions and presence records

To do the spatial join we use the command `ST_Intersects` of PostGIS to find the ecoregion over which every record of plantae can be found. Instead of updating `plantae`, here I create a new table named `plantae_ecoregions`

```{sql connection=postgresql.connection}
CREATE TABLE plantae_ecoregions AS
SELECT 
plantae.*, 
ecoregions.ecoregion, 
ecoregions.realm, 
ecoregions.biome 
FROM ecoregions
JOIN plantae 
ON ST_Intersects(ecoregions.geom, plantae.geom);
```



# References

Olson, D. M., Dinerstein, E., Wikramanayake, E. D., Burgess, N. D., Powell, G. V. N., Underwood, E. C., D'Amico, J. A., Itoua, I., Strand, H. E., Morrison, J. C., Loucks, C. J., Allnutt, T. F., Ricketts, T. H., Kura, Y., Lamoreux, J. F., Wettengel, W. W., Hedao, P., Kassem, K. R. 2001. Terrestrial ecoregions of the world: a new map of life on Earth. Bioscience 51(11):933-938.
